# Example: google_key_path = /files/service-account-json
#
# Variable: AIRFLOW__API__GOOGLE_KEY_PATH
#
# google_key_path = 

# Used in response to a preflight request to indicate which HTTP
# headers can be used when making the actual request. This header is
# the server side response to the browser's
# Access-Control-Request-Headers header.
#
# Variable: AIRFLOW__API__ACCESS_CONTROL_ALLOW_HEADERS
#
# access_control_allow_headers = 

# Specifies the method or methods allowed when accessing the resource.
#
# Variable: AIRFLOW__API__ACCESS_CONTROL_ALLOW_METHODS
#
# access_control_allow_methods = 

# Indicates whether the response can be shared with requesting code from the given origins.
# Separate URLs with space.
#
# Variable: AIRFLOW__API__ACCESS_CONTROL_ALLOW_ORIGINS
#
# access_control_allow_origins = 

# Indicates whether the *xcomEntries* endpoint supports the *deserialize*
# flag. If set to False, setting this flag in a request would result in a
# 400 Bad Request error.
#
# Variable: AIRFLOW__API__ENABLE_XCOM_DESERIALIZE_SUPPORT
#
# enable_xcom_deserialize_support = False

[lineage]
# what lineage backend to use
#
# Variable: AIRFLOW__LINEAGE__BACKEND
#
# backend = 

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
#
# Variable: AIRFLOW__OPERATORS__DEFAULT_OWNER
#
# default_owner = airflow

# The default value of attribute "deferrable" in operators and sensors.
#
# Variable: AIRFLOW__OPERATORS__DEFAULT_DEFERRABLE
#
# default_deferrable = false

#
# Variable: AIRFLOW__OPERATORS__DEFAULT_CPUS
#
# default_cpus = 1

#
# Variable: AIRFLOW__OPERATORS__DEFAULT_RAM
#
# default_ram = 512

#
# Variable: AIRFLOW__OPERATORS__DEFAULT_DISK
#
# default_disk = 512

#
# Variable: AIRFLOW__OPERATORS__DEFAULT_GPUS
#
# default_gpus = 0

# Default queue that tasks get assigned to and that worker listen on.
#
# Variable: AIRFLOW__OPERATORS__DEFAULT_QUEUE
#
# default_queue = default

# Is allowed to pass additional/unused arguments (args, kwargs) to the BaseOperator operator.
# If set to False, an exception will be thrown, otherwise only the console message will be displayed.
#
# Variable: AIRFLOW__OPERATORS__ALLOW_ILLEGAL_ARGUMENTS
#
# allow_illegal_arguments = False

[webserver]
# The message displayed when a user attempts to execute actions beyond their authorised privileges.
#
# Variable: AIRFLOW__WEBSERVER__ACCESS_DENIED_MESSAGE
#
# access_denied_message = Access is Denied

# Path of webserver config file used for configuring the webserver parameters
#
# Variable: AIRFLOW__WEBSERVER__CONFIG_FILE
#
# config_file = /root/airflow/webserver_config.py

# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
#
# Variable: AIRFLOW__WEBSERVER__BASE_URL
#
# base_url = http://localhost:8080

# Default timezone to display all dates in the UI, can be UTC, system, or
# any IANA timezone string (e.g. Europe/Amsterdam). If left empty the
# default value of core/default_timezone will be used
#
# Example: default_ui_timezone = America/New_York
#
# Variable: AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE
#
# default_ui_timezone = UTC

# The ip specified when starting the web server
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_HOST
#
# web_server_host = 0.0.0.0

# The port on which to run the web server
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_PORT
#
# web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT
#
# web_server_ssl_cert = 

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY
#
# web_server_ssl_key = 

# The type of backend used to store web session data, can be `database` or `securecookie`. For the
# `database` backend, sessions are store in the database (in `session` table) and they can be
# managed there (for example when you reset password of the user, all sessions for that user are
# deleted). For the `securecookie` backend, sessions are stored in encrypted cookies on the client
# side. The `securecookie` mechanism is 'lighter' than database backend, but sessions are not deleted
# when you reset password of the user, which means that other than waiting for expiry time, the only
# way to invalidate all sessions for a user is to change secret_key and restart webserver (which
# also invalidates and logs out all other user's sessions).
# 
# When you are using `database` backend, make sure to keep your database session table small
# by periodically running `airflow db clean --table session` command, especially if you have
# automated API calls that will create a new session for each call rather than reuse the sessions
# stored in browser cookies.
#
# Example: session_backend = securecookie
#
# Variable: AIRFLOW__WEBSERVER__SESSION_BACKEND
#
# session_backend = database

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT
#
# web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
#
# Variable: AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT
#
# web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
#
# Variable: AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE
#
# worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
#
# Variable: AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL
#
# worker_refresh_interval = 6000

# If set to True, Airflow will track files in plugins_folder directory. When it detects changes,
# then reload the gunicorn. If set to True, gunicorn starts without preloading, which is slower, uses
# more memory, and may cause race conditions. Avoid setting this to True in production.
#
# Variable: AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE
#
# reload_on_plugin_change = False

# Secret key used to run your flask app. It should be as random as possible. However, when running
# more than 1 instances of webserver, make sure all of them use the same ``secret_key`` otherwise
# one of them will error with "CSRF session token is missing".
# The webserver key is also used to authorize requests to Celery workers when logs are retrieved.
# The token generated using the secret key has a short expiry time though - make sure that time on
# ALL the machines that you run airflow components on is synchronized (for example using ntpd)
# otherwise you might get "forbidden" errors when the logs are accessed.
#
# Variable: AIRFLOW__WEBSERVER__SECRET_KEY
#
# secret_key = eCG8rt9yfhIrdC4lDmUYSQ==

# Number of workers to run the Gunicorn web server
#
# Variable: AIRFLOW__WEBSERVER__WORKERS
#
# workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent. Note when using gevent you might also want to set the
# "_AIRFLOW_PATCH_GEVENT" environment variable to "1" to make sure gevent patching is done as
# early as possible.
#
# Variable: AIRFLOW__WEBSERVER__WORKER_CLASS
#
# worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
#
# Variable: AIRFLOW__WEBSERVER__ACCESS_LOGFILE
#
# access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
#
# Variable: AIRFLOW__WEBSERVER__ERROR_LOGFILE
#
# error_logfile = -

# Access log format for gunicorn webserver.
# default format is %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s "%%(f)s" "%%(a)s"
# documentation - https://docs.gunicorn.org/en/stable/settings.html#access-log-format
#
# Variable: AIRFLOW__WEBSERVER__ACCESS_LOGFORMAT
#
# access_logformat = 

# Expose the configuration file in the web server. Set to "non-sensitive-only" to show all values
# except those that have security implications. "True" shows all values. "False" hides the
# configuration completely.
#
# Variable: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
#
# expose_config = False

# Expose hostname in the web server
#
# Variable: AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME
#
# expose_hostname = False

# Expose stacktrace in the web server
#
# Variable: AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE
#
# expose_stacktrace = False

# Default DAG view. Valid values are: ``grid``, ``graph``, ``duration``, ``gantt``, ``landing_times``
#
# Variable: AIRFLOW__WEBSERVER__DAG_DEFAULT_VIEW
#
# dag_default_view = grid

# Default DAG orientation. Valid values are:
# ``LR`` (Left->Right), ``TB`` (Top->Bottom), ``RL`` (Right->Left), ``BT`` (Bottom->Top)
#
# Variable: AIRFLOW__WEBSERVER__DAG_ORIENTATION
#
# dag_orientation = LR

# Sorting order in grid view. Valid values are: ``topological``, ``hierarchical_alphabetical``
#
# Variable: AIRFLOW__WEBSERVER__GRID_VIEW_SORTING_ORDER
#
# grid_view_sorting_order = topological

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
#
# Variable: AIRFLOW__WEBSERVER__LOG_FETCH_TIMEOUT_SEC
#
# log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
#
# Variable: AIRFLOW__WEBSERVER__LOG_FETCH_DELAY_SEC
#
# log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
#
# Variable: AIRFLOW__WEBSERVER__LOG_AUTO_TAILING_OFFSET
#
# log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
#
# Variable: AIRFLOW__WEBSERVER__LOG_ANIMATION_SPEED
#
# log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
#
# Variable: AIRFLOW__WEBSERVER__HIDE_PAUSED_DAGS_BY_DEFAULT
#
# hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
#
# Variable: AIRFLOW__WEBSERVER__PAGE_SIZE
#
# page_size = 100

# Define the color of navigation bar
#
# Variable: AIRFLOW__WEBSERVER__NAVBAR_COLOR
#
# navbar_color = #fff

# Default dagrun to show in UI
#
# Variable: AIRFLOW__WEBSERVER__DEFAULT_DAG_RUN_DISPLAY_NUMBER
#
# default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
#
# Variable: AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX
#
# enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
#
# Variable: AIRFLOW__WEBSERVER__PROXY_FIX_X_FOR
#
# proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
#
# Variable: AIRFLOW__WEBSERVER__PROXY_FIX_X_PROTO
#
# proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
#
# Variable: AIRFLOW__WEBSERVER__PROXY_FIX_X_HOST
#
# proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
#
# Variable: AIRFLOW__WEBSERVER__PROXY_FIX_X_PORT
#
# proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
#
# Variable: AIRFLOW__WEBSERVER__PROXY_FIX_X_PREFIX
#
# proxy_fix_x_prefix = 1

# Set secure flag on session cookie
#
# Variable: AIRFLOW__WEBSERVER__COOKIE_SECURE
#
# cookie_secure = False

# Set samesite policy on session cookie
#
# Variable: AIRFLOW__WEBSERVER__COOKIE_SAMESITE
#
# cookie_samesite = Lax

# Default setting for wrap toggle on DAG code and TI log views.
#
# Variable: AIRFLOW__WEBSERVER__DEFAULT_WRAP
#
# default_wrap = False

# Allow the UI to be rendered in a frame
#
# Variable: AIRFLOW__WEBSERVER__X_FRAME_ENABLED
#
# x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
#
# Variable: AIRFLOW__WEBSERVER__ANALYTICS_TOOL
#
# analytics_tool = 

# Unique ID of your account in the analytics tool
#
# Variable: AIRFLOW__WEBSERVER__ANALYTICS_ID
#
# analytics_id = 

# 'Recent Tasks' stats will show for old DagRuns if set
#
# Variable: AIRFLOW__WEBSERVER__SHOW_RECENT_STATS_FOR_COMPLETED_RUNS
#
# show_recent_stats_for_completed_runs = True

# Update FAB permissions and sync security manager roles
# on webserver startup
#
# Variable: AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS
#
# update_fab_perms = True

# The UI cookie lifetime in minutes. User will be logged out from UI after
# ``session_lifetime_minutes`` of non-activity
#
# Variable: AIRFLOW__WEBSERVER__SESSION_LIFETIME_MINUTES
#
# session_lifetime_minutes = 43200

# Sets a custom page title for the DAGs overview page and site title for all pages
#
# Variable: AIRFLOW__WEBSERVER__INSTANCE_NAME
#
# instance_name = 

# Whether the custom page title for the DAGs overview page contains any Markup language
#
# Variable: AIRFLOW__WEBSERVER__INSTANCE_NAME_HAS_MARKUP
#
# instance_name_has_markup = False

# How frequently, in seconds, the DAG data will auto-refresh in graph or grid view
# when auto-refresh is turned on
#
# Variable: AIRFLOW__WEBSERVER__AUTO_REFRESH_INTERVAL
#
# auto_refresh_interval = 3

# Boolean for displaying warning for publicly viewable deployment
#
# Variable: AIRFLOW__WEBSERVER__WARN_DEPLOYMENT_EXPOSURE
#
# warn_deployment_exposure = True

# Comma separated string of view events to exclude from dag audit view.
# All other events will be added minus the ones passed here.
# The audit logs in the db will not be affected by this parameter.
#
# Variable: AIRFLOW__WEBSERVER__AUDIT_VIEW_EXCLUDED_EVENTS
#
# audit_view_excluded_events = gantt,landing_times,tries,duration,calendar,graph,grid,tree,tree_data

# Comma separated string of view events to include in dag audit view.
# If passed, only these events will populate the dag audit view.
# The audit logs in the db will not be affected by this parameter.
#
# Example: audit_view_included_events = dagrun_cleared,failed
#
# Variable: AIRFLOW__WEBSERVER__AUDIT_VIEW_INCLUDED_EVENTS
#
# audit_view_included_events = 

# Boolean for running SwaggerUI in the webserver.
#
# Variable: AIRFLOW__WEBSERVER__ENABLE_SWAGGER_UI
#
# enable_swagger_ui = True

# Boolean for running Internal API in the webserver.
#
# Variable: AIRFLOW__WEBSERVER__RUN_INTERNAL_API
#
# run_internal_api = False

# Boolean for enabling rate limiting on authentication endpoints.
#
# Variable: AIRFLOW__WEBSERVER__AUTH_RATE_LIMITED
#
# auth_rate_limited = True

# Rate limit for authentication endpoints.
#
# Variable: AIRFLOW__WEBSERVER__AUTH_RATE_LIMIT
#
# auth_rate_limit = 5 per 40 second

# The caching algorithm used by the webserver. Must be a valid hashlib function name.
#
# Example: caching_hash_method = sha256
#
# Variable: AIRFLOW__WEBSERVER__CACHING_HASH_METHOD
#
# caching_hash_method = md5

# Behavior of the trigger DAG run button for DAGs without params. False to skip and trigger
# without displaying a form to add a dag_run.conf, True to always display the form.
# The form is displayed always if parameters are defined.
#
# Variable: AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS
#
# show_trigger_form_if_no_params = False

[email]
# Configuration email backend and whether to
# send email alerts on retry or failure

# Email backend to use
#
# Variable: AIRFLOW__EMAIL__EMAIL_BACKEND
#
# email_backend = airflow.utils.email.send_email_smtp

# Email connection to use
#
# Variable: AIRFLOW__EMAIL__EMAIL_CONN_ID
#
# email_conn_id = smtp_default

# Whether email alerts should be sent when a task is retried
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_RETRY
#
# default_email_on_retry = True

# Whether email alerts should be sent when a task failed
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_FAILURE
#
# default_email_on_failure = True

# File that will be used as the template for Email subject (which will be rendered using Jinja2).
# If not set, Airflow uses a base template.
#
# Example: subject_template = /path/to/my_subject_template_file
#
# Variable: AIRFLOW__EMAIL__SUBJECT_TEMPLATE
#
# subject_template = 

# File that will be used as the template for Email content (which will be rendered using Jinja2).
# If not set, Airflow uses a base template.
#
# Example: html_content_template = /path/to/my_html_content_template_file
#
# Variable: AIRFLOW__EMAIL__HTML_CONTENT_TEMPLATE
#
# html_content_template = 

# Email address that will be used as sender address.
# It can either be raw email or the complete address in a format ``Sender Name <sender@email.com>``
#
# Example: from_email = Airflow <airflow@example.com>
#
# Variable: AIRFLOW__EMAIL__FROM_EMAIL
#
# from_email = 

# ssl context to use when using SMTP and IMAP SSL connections. By default, the context is "default"
# which sets it to ``ssl.create_default_context()`` which provides the right balance between
# compatibility and security, it however requires that certificates in your operating system are
# updated and that SMTP/IMAP servers of yours have valid certificates that have corresponding public
# keys installed on your machines. You can switch it to "none" if you want to disable checking
# of the certificates, but it is not recommended as it allows MITM (man-in-the-middle) attacks
# if your infrastructure is not sufficiently secured. It should only be set temporarily while you
# are fixing your certificate configuration. This can be typically done by upgrading to newer
# version of the operating system you run Airflow components on,by upgrading/refreshing proper
# certificates in the OS or by updating certificates for your mail servers.
#
# Example: ssl_context = default
#
# Variable: AIRFLOW__EMAIL__SSL_CONTEXT
#
# ssl_context = default

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here

#
# Variable: AIRFLOW__SMTP__SMTP_HOST
#
# smtp_host = localhost

#
# Variable: AIRFLOW__SMTP__SMTP_STARTTLS
#
# smtp_starttls = True

#
# Variable: AIRFLOW__SMTP__SMTP_SSL
#
# smtp_ssl = False

#
# Example: smtp_user = airflow
#
# Variable: AIRFLOW__SMTP__SMTP_USER
#
# smtp_user = 

#
# Example: smtp_password = airflow
#
# Variable: AIRFLOW__SMTP__SMTP_PASSWORD
#
# smtp_password = 

#
# Variable: AIRFLOW__SMTP__SMTP_PORT
#
# smtp_port = 25

#
# Variable: AIRFLOW__SMTP__SMTP_MAIL_FROM
#
# smtp_mail_from = airflow@example.com

#
# Variable: AIRFLOW__SMTP__SMTP_TIMEOUT
#
# smtp_timeout = 30

#
# Variable: AIRFLOW__SMTP__SMTP_RETRY_LIMIT
#
# smtp_retry_limit = 5

[sentry]
# Sentry (https://docs.sentry.io) integration. Here you can supply
# additional configuration options based on the Python platform. See:
# https://docs.sentry.io/error-reporting/configuration/?platform=python.
# Unsupported options: ``integrations``, ``in_app_include``, ``in_app_exclude``,
# ``ignore_errors``, ``before_breadcrumb``, ``transport``.

# Enable error reporting to Sentry
#
# Variable: AIRFLOW__SENTRY__SENTRY_ON
#
# sentry_on = false

#
# Variable: AIRFLOW__SENTRY__SENTRY_DSN
#
# sentry_dsn = 

# Dotted path to a before_send function that the sentry SDK should be configured to use.
#
# Variable: AIRFLOW__SENTRY__BEFORE_SEND
#
# before_send = 

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
#
# Variable: AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC
#
# job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC
#
# scheduler_heartbeat_sec = 5

# The frequency (in seconds) at which the LocalTaskJob should send heartbeat signals to the
# scheduler to notify it's still alive. If this value is set to 0, the heartbeat interval will default
# to the value of scheduler_zombie_task_threshold.
#
# Variable: AIRFLOW__SCHEDULER__LOCAL_TASK_JOB_HEARTBEAT_SEC
#
# local_task_job_heartbeat_sec = 0

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
#
# Variable: AIRFLOW__SCHEDULER__NUM_RUNS
#
# num_runs = -1

# Controls how long the scheduler will sleep between loops, but if there was nothing to do
# in the loop. i.e. if it scheduled something then it will start the next loop
# iteration straight away.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_IDLE_SLEEP_TIME
#
# scheduler_idle_sleep_time = 1

# Number of seconds after which a DAG file is parsed. The DAG file is parsed every
# ``min_file_process_interval`` number of seconds. Updates to DAGs are reflected after
# this interval. Keeping this number low will increase CPU usage.
#
# Variable: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
#
# min_file_process_interval = 30

# How often (in seconds) to check for stale DAGs (DAGs which are no longer present in
# the expected files) which should be deactivated, as well as datasets that are no longer
# referenced and should be marked as orphaned.
#
# Variable: AIRFLOW__SCHEDULER__PARSING_CLEANUP_INTERVAL
#
# parsing_cleanup_interval = 60

# How long (in seconds) to wait after we have re-parsed a DAG file before deactivating stale
# DAGs (DAGs which are no longer present in the expected files). The reason why we need
# this threshold is to account for the time between when the file is parsed and when the
# DAG is loaded. The absolute maximum that this could take is `dag_file_processor_timeout`,
# but when you have a long timeout configured, it results in a significant delay in the
# deactivation of stale dags.
#
# Variable: AIRFLOW__SCHEDULER__STALE_DAG_THRESHOLD
#
# stale_dag_threshold = 50

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
#
# Variable: AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL
#
# dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
#
# Variable: AIRFLOW__SCHEDULER__PRINT_STATS_INTERVAL
#
# print_stats_interval = 30

# How often (in seconds) should pool usage stats be sent to StatsD (if statsd_on is enabled)
#
# Variable: AIRFLOW__SCHEDULER__POOL_METRICS_INTERVAL
#
# pool_metrics_interval = 5.0

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint and in `airflow jobs check` CLI
# for SchedulerJob.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD
#
# scheduler_health_check_threshold = 30

# When you start a scheduler, airflow starts a tiny web server
# subprocess to serve a health check if this is set to True
#
# Variable: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
#
# enable_health_check = False

# When you start a scheduler, airflow starts a tiny web server
# subprocess to serve a health check on this port
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_SERVER_PORT
#
# scheduler_health_check_server_port = 8974

# How often (in seconds) should the scheduler check for orphaned tasks and SchedulerJobs
#
# Variable: AIRFLOW__SCHEDULER__ORPHANED_TASKS_CHECK_INTERVAL
#
# orphaned_tasks_check_interval = 300.0

#
# Variable: AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY
#
# child_process_log_directory = /root/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD
#
# scheduler_zombie_task_threshold = 300

# How often (in seconds) should the scheduler check for zombie tasks.
#
# Variable: AIRFLOW__SCHEDULER__ZOMBIE_DETECTION_INTERVAL
#
# zombie_detection_interval = 10.0

# Turn off scheduler catchup by setting this to ``False``.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is ``False``,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
#
# Variable: AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT
#
# catchup_by_default = True

# Setting this to True will make first task instance of a task
# ignore depends_on_past setting. A task instance will be considered
# as the first task instance of a task when there is no task instance
# in the DB with an execution_date earlier than it., i.e. no manual marking
# success will be needed for a newly added task to be scheduled.
#
# Variable: AIRFLOW__SCHEDULER__IGNORE_FIRST_DEPENDS_ON_PAST_BY_DEFAULT
#
# ignore_first_depends_on_past_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# This should not be greater than ``core.parallelism``.
# If this is too high, SQL query performance may be impacted by
# complexity of query predicate, and/or excessive locking.
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 to use the value of ``core.parallelism``
#
# Variable: AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY
#
# max_tis_per_query = 16

# Should the scheduler issue ``SELECT ... FOR UPDATE`` in relevant queries.
# If this is set to False then you should not run more than a single
# scheduler at once
#
# Variable: AIRFLOW__SCHEDULER__USE_ROW_LEVEL_LOCKING
#
# use_row_level_locking = True

# Max number of DAGs to create DagRuns for per scheduler loop.
#
# Variable: AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP
#
# max_dagruns_to_create_per_loop = 10

# How many DagRuns should a scheduler examine (and lock) when scheduling
# and queuing tasks.
#
# Variable: AIRFLOW__SCHEDULER__MAX_DAGRUNS_PER_LOOP_TO_SCHEDULE
#
# max_dagruns_per_loop_to_schedule = 20

# Should the Task supervisor process perform a "mini scheduler" to attempt to schedule more tasks of the
# same DAG. Leaving this on will mean tasks in the same DAG execute quicker, but might starve out other
# dags in some circumstances
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION
#
# schedule_after_task_execution = True

# The scheduler reads dag files to extract the airflow modules that are going to be used,
# and imports them ahead of time to avoid having to re-do it for each parsing process.
# This flag can be set to False to disable this behavior in case an airflow module needs to be freshly
# imported each time (at the cost of increased DAG parsing time).
#
# Variable: AIRFLOW__SCHEDULER__PARSING_PRE_IMPORT_MODULES
#
# parsing_pre_import_modules = True

# The scheduler can run multiple processes in parallel to parse dags.
# This defines how many processes will run.
#
# Variable: AIRFLOW__SCHEDULER__PARSING_PROCESSES
#
# parsing_processes = 2

# One of ``modified_time``, ``random_seeded_by_host`` and ``alphabetical``.
# The scheduler will list and sort the dag files to decide the parsing order.
# 
# * ``modified_time``: Sort by modified time of the files. This is useful on large scale to parse the
#   recently modified DAGs first.
# * ``random_seeded_by_host``: Sort randomly across multiple Schedulers but with same order on the
#   same host. This is useful when running with Scheduler in HA mode where each scheduler can
#   parse different DAG files.
# * ``alphabetical``: Sort by filename
#
# Variable: AIRFLOW__SCHEDULER__FILE_PARSING_SORT_MODE
#
# file_parsing_sort_mode = modified_time

# Whether the dag processor is running as a standalone process or it is a subprocess of a scheduler
# job.
#
# Variable: AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR
#
# standalone_dag_processor = False

# Only applicable if `[scheduler]standalone_dag_processor` is true and  callbacks are stored
# in database. Contains maximum number of callbacks that are fetched during a single loop.
#
# Variable: AIRFLOW__SCHEDULER__MAX_CALLBACKS_PER_LOOP
#
# max_callbacks_per_loop = 20

# Only applicable if `[scheduler]standalone_dag_processor` is true.
# Time in seconds after which dags, which were not updated by Dag Processor are deactivated.
#
# Variable: AIRFLOW__SCHEDULER__DAG_STALE_NOT_SEEN_DURATION
#
# dag_stale_not_seen_duration = 600

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
#
# Variable: AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE
#
# use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
#
# Variable: AIRFLOW__SCHEDULER__ALLOW_TRIGGER_IN_FUTURE
#
# allow_trigger_in_future = False

# How often to check for expired trigger requests that have not run yet.
#
# Variable: AIRFLOW__SCHEDULER__TRIGGER_TIMEOUT_CHECK_INTERVAL
#
# trigger_timeout_check_interval = 15

# Amount of time a task can be in the queued state before being retried or set to failed.
#
# Variable: AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT
#
# task_queued_timeout = 600.0

# How often to check for tasks that have been in the queued state for
# longer than `[scheduler] task_queued_timeout`.
#
# Variable: AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT_CHECK_INTERVAL
#
# task_queued_timeout_check_interval = 120.0

# The run_id pattern used to verify the validity of user input to the run_id parameter when
# triggering a DAG. This pattern cannot change the pattern used by scheduler to generate run_id
# for scheduled DAG runs or DAG runs triggered without changing the run_id parameter.
#
# Variable: AIRFLOW__SCHEDULER__ALLOWED_RUN_ID_PATTERN
#
# allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$

[triggerer]
# How many triggers a single Triggerer will run at once, by default.
#
# Variable: AIRFLOW__TRIGGERER__DEFAULT_CAPACITY
#
# default_capacity = 1000

# How often to heartbeat the Triggerer job to ensure it hasn't been killed.
#
# Variable: AIRFLOW__TRIGGERER__JOB_HEARTBEAT_SEC
#
# job_heartbeat_sec = 5

# If the last triggerer heartbeat happened more than triggerer_health_check_threshold
# ago (in seconds), triggerer is considered unhealthy.
# This is used by the health check in the "/health" endpoint and in `airflow jobs check` CLI
# for TriggererJob.
#
# Variable: AIRFLOW__TRIGGERER__TRIGGERER_HEALTH_CHECK_THRESHOLD
#
# triggerer_health_check_threshold = 30

[kerberos]
#
# Variable: AIRFLOW__KERBEROS__CCACHE
#
# ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
#
# Variable: AIRFLOW__KERBEROS__PRINCIPAL
#
# principal = airflow

#
# Variable: AIRFLOW__KERBEROS__REINIT_FREQUENCY
#
# reinit_frequency = 3600

#
# Variable: AIRFLOW__KERBEROS__KINIT_PATH
#
# kinit_path = kinit

#
# Variable: AIRFLOW__KERBEROS__KEYTAB
#
# keytab = airflow.keytab

# Allow to disable ticket forwardability.
#
# Variable: AIRFLOW__KERBEROS__FORWARDABLE
#
# forwardable = True

# Allow to remove source IP from token, useful when using token behind NATted Docker host.
#
# Variable: AIRFLOW__KERBEROS__INCLUDE_IP
#
# include_ip = True

[sensors]
# Sensor default timeout, 7 days by default (7 * 24 * 60 * 60).
#
# Variable: AIRFLOW__SENSORS__DEFAULT_TIMEOUT
#
# default_timeout = 604800

[imap]
# Options for IMAP provider.

# ssl_context =